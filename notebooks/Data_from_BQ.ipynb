{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b45051c-dae4-41d5-a941-55466c61df64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # this allows simple navigation of folders in the notebook\n",
    "import pandas as pd # this loads Pandas\n",
    "import numpy as np # This loads NumPy, used to do maths in the notebook\n",
    "from ebmdatalab import bq # this loads the bennett institute bigquery function\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "928174c5-55a7-4049-aea4-d7731d0879ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ../data/data_chunk_0.csv.gz, size: 41.37 MB\n",
      "Saved ../data/data_chunk_1.csv.gz, size: 41.57 MB\n",
      "Saved ../data/data_chunk_2.csv.gz, size: 41.36 MB\n",
      "Saved ../data/data_chunk_3.csv.gz, size: 41.60 MB\n",
      "Saved ../data/data_chunk_4.csv.gz, size: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# get prescribing data - for reference only - this won't work unless you have a BQ query account\n",
    "sql = '''\n",
    "SELECT DATE(month) AS month, pct, practice, bnf_name, bnf_code, items, quantity, net_cost, actual_cost\n",
    "FROM hscic.normalised_prescribing\n",
    "WHERE\n",
    "(bnf_code like '0101%' or bnf_code like '0103%')\n",
    "AND\n",
    "month >='2019-01-01'\n",
    "'''\n",
    "tempfile = os.path.join(\"..\",\"data\",\"omep_df_temp.csv\") \n",
    "omep_df = bq.cached_read(sql, csv_path=tempfile, use_cache=True)\n",
    "\n",
    "# chunk data go <100mb for GitHub\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the maximum compressed file size in bytes (100 MB)\n",
    "max_compressed_file_size = 100 * 1024 * 1024\n",
    "\n",
    "# Initialize variables\n",
    "file_index = 0\n",
    "total_rows = len(omep_df)\n",
    "rows_per_chunk = total_rows // 4  # Start with an approximate number of rows per file\n",
    "\n",
    "# Start writing the DataFrame to compressed CSV files\n",
    "for start in range(0, total_rows, rows_per_chunk):\n",
    "    while True:\n",
    "        # Select a chunk of the DataFrame\n",
    "        omep_df_chunk = omep_df.iloc[start:start + rows_per_chunk]\n",
    "\n",
    "        # Save the chunk to a compressed CSV file\n",
    "        file_name = os.path.join(\"..\",\"data\",f\"omep_df_chunk_{file_index}.csv.gz\")\n",
    "        omep_df_chunk.to_csv(file_name, index=False, compression='gzip')\n",
    "\n",
    "        # Check the size of the compressed file\n",
    "        compressed_file_size = os.path.getsize(file_name)\n",
    "\n",
    "        # If the file size is under the limit, proceed to the next chunk\n",
    "        if compressed_file_size < max_compressed_file_size:\n",
    "            print(f\"Saved {file_name}, size: {compressed_file_size / (1024 * 1024):.2f} MB\")\n",
    "            file_index += 1\n",
    "            break\n",
    "        else:\n",
    "            # If the file size exceeds the limit, reduce the chunk size and try again\n",
    "            rows_per_chunk = rows_per_chunk // 2\n",
    "\n",
    "            # Remove the oversized file to try again\n",
    "            os.remove(file_name)\n",
    "\n",
    "            # Check if rows_per_chunk has become too small (to avoid infinite loop)\n",
    "            if rows_per_chunk == 0:\n",
    "                raise ValueError(\"Cannot compress to below 100 MB, consider a larger chunk size.\")\n",
    "\n",
    "\n",
    "#delete the temp file\n",
    "os.remove(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb7041db-a5c6-4d22-b8c1-912836855f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|\u001b[32m█████████████████████████████████████████████████████████████\u001b[0m|\u001b[0m\n",
      "Downloading: 100%|\u001b[32m█████████████████████████████████████████████████████████████\u001b[0m|\u001b[0m\n",
      "Downloading: 100%|\u001b[32m█████████████████████████████████████████████████████████████\u001b[0m|\u001b[0m\n",
      "Downloading: 100%|\u001b[32m█████████████████████████████████████████████████████████████\u001b[0m|\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# get related data sets\n",
    "#BNF data\n",
    "sql = 'SELECT * FROM hscic.bnf'\n",
    "exportfile = os.path.join(\"..\",\"data\",\"bnf_df.csv\") \n",
    "bnf_df = bq.cached_read(sql, csv_path=exportfile, use_cache=False)\n",
    "\n",
    "#Practice data\n",
    "sql = 'SELECT * FROM hscic.practices'\n",
    "exportfile = os.path.join(\"..\",\"data\",\"practices_df.csv\")\n",
    "practices_df = bq.cached_read(sql, csv_path=exportfile, use_cache=False)\n",
    "\n",
    "# Practice stats data\n",
    "sql = 'SELECT DATE(month) AS month, total_list_size, practice FROM hscic.practice_statistics'\n",
    "exportfile = os.path.join(\"..\",\"data\",\"statistics_df.csv\") \n",
    "statistics_df = bq.cached_read(sql, csv_path=exportfile, use_cache=False)\n",
    "\n",
    "# CCG/SICBL data\n",
    "sql = 'SELECT * from hscic.ccgs'\n",
    "exportfile = os.path.join(\"..\",\"data\",\"ccg_df.csv\") \n",
    "statistics_df = bq.cached_read(sql, csv_path=exportfile, use_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18e5cee-73cd-4499-bb3f-8c7388494945",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
